{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLjpwPwE2/XICZYMwnqTTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karenbennis/BigData/blob/master/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLWbpXXXqGzy"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnp7GGtYqMO3"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Yelp_NLP\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjs0xXSsqOIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "8ad3908c-27dd-45ae-8c1c-52edc3ae0eff"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|   class|                text|\n",
            "+--------+--------------------+\n",
            "|positive|Wow... Loved this...|\n",
            "|negative|  Crust is not good.|\n",
            "|negative|Not tasty and the...|\n",
            "|positive|Stopped by during...|\n",
            "|positive|The selection on ...|\n",
            "|negative|Now I am getting ...|\n",
            "|negative|Honeslty it didn'...|\n",
            "|negative|The potatoes were...|\n",
            "|positive|The fries were gr...|\n",
            "|positive|      A great touch.|\n",
            "|positive|Service was very ...|\n",
            "|negative|  Would not go back.|\n",
            "|negative|The cashier had n...|\n",
            "|positive|I tried the Cape ...|\n",
            "|negative|I was disgusted b...|\n",
            "|negative|I was shocked bec...|\n",
            "|positive| Highly recommended.|\n",
            "|negative|Waitress was a li...|\n",
            "|negative|This place is not...|\n",
            "|negative|did not like at all.|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WrdXcr6qRRv"
      },
      "source": [
        "# Import functions\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvgX11H7qTqt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "9ecf8b78-30f7-4fc5-b8cf-ac6635aa9827"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature\n",
        "data_df = df.withColumn('length', length(df['text']))\n",
        "data_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pncBE74xqWFR"
      },
      "source": [
        "# Create all the features to the data set\n",
        "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fZH7XnQqcS0"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "# Create feature vectors\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pczuk5mqfWz"
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CcTpnJiqh9R"
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(data_df)\n",
        "cleaned = cleaner.transform(data_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NpHsWebq2SJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "8cc30219-6e4c-4146-ab69-862df0e14335"
      },
      "source": [
        "# Show label and resulting features\n",
        "cleaned.select(['label', 'features']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(262145,[33933,69...|\n",
            "|  1.0|(262145,[15889,13...|\n",
            "|  1.0|(262145,[25570,63...|\n",
            "|  0.0|(262145,[6286,272...|\n",
            "|  0.0|(262145,[6979,255...|\n",
            "|  1.0|(262145,[24417,24...|\n",
            "|  1.0|(262145,[12084,48...|\n",
            "|  1.0|(262145,[3645,963...|\n",
            "|  0.0|(262145,[53777,10...|\n",
            "|  0.0|(262145,[138356,2...|\n",
            "|  0.0|(262145,[24113,25...|\n",
            "|  1.0|(262145,[68867,13...|\n",
            "|  1.0|(262145,[24417,36...|\n",
            "|  0.0|(262145,[18098,24...|\n",
            "|  1.0|(262145,[24417,25...|\n",
            "|  1.0|(262145,[24417,25...|\n",
            "|  0.0|(262145,[31704,21...|\n",
            "|  1.0|(262145,[25570,27...|\n",
            "|  1.0|(262145,[12329,15...|\n",
            "|  1.0|(262145,[8287,139...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpv2uSyyrI3y"
      },
      "source": [
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r2wugE6rUXL"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB0vTb7vrsI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "7f1616c1-4e98-4cb8-a173-03eac165a4a6"
      },
      "source": [
        "# Transform the model with the testing data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative| \"As for the \"\"mains|    19|  1.0|[\"as, for, the, \"...|      [\"as, \"\"mains]|(262144,[16332,10...|(262144,[16332,10...|(262145,[16332,10...|[-235.05580770267...|[0.03900923601832...|       1.0|\n",
            "|negative|\"I don't know wha...|    85|  1.0|[\"i, don't, know,...|[\"i, know, big, d...|(262144,[8478,158...|(262144,[8478,158...|(262145,[8478,158...|[-876.28450835078...|[5.82709985215757...|       1.0|\n",
            "|negative|\"like the other r...|    82|  1.0|[\"like, the, othe...|[\"like, reviewer,...|(262144,[22808,61...|(262144,[22808,61...|(262145,[22808,61...|[-930.20054206924...|[5.67229347354298...|       1.0|\n",
            "|negative| \"not even a \"\"hello|    19|  1.0|[\"not, even, a, \"...|[\"not, even, \"\"he...|(262144,[174966,2...|(262144,[174966,2...|(262145,[174966,2...|[-259.46346332024...|[0.19845430741939...|       1.0|\n",
            "|negative|              #NAME?|     6|  1.0|            [#name?]|            [#name?]|(262144,[75443],[...|(262144,[75443],[...|(262145,[75443,26...|[-70.880216880965...|[0.93675076661110...|       0.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOWeTAyLr4zt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "36be2dc4-1096-44ba-94c3-25c5116bd231"
      },
      "source": [
        "# Use the Class Evaluator for a cleaner description\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model at predicting reviews was: 0.718984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7YgPwW0xG7T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}